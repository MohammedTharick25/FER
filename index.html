<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Facial Emotion Recognition</title>
    <script defer src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/face-api.js"></script>
    <style>
        body { font-family: Arial, sans-serif; text-align: center; margin-top: 20px; }
        #video { width: 400px; border: 2px solid black; }
        #captureButton { padding: 10px 20px; font-size: 16px; cursor: pointer; margin: 10px; }
        h2 { margin-top: 20px; }
    </style>
</head>
<body>

    <h1>Facial Emotion Recognition</h1>
    
    <h2>Use Your Webcam</h2>
    <video id="video" autoplay></video>
    <br>
    <button id="captureButton">Capture & Predict</button>
    <h2 id="result">Emotion: -</h2>

    <script>
        const video = document.getElementById('video');
        const captureButton = document.getElementById('captureButton');
        const resultText = document.getElementById('result');

        // Load Face-api.js models
        async function loadModels() {
            await faceapi.nets.tinyFaceDetector.loadFromUri('https://cdn.jsdelivr.net/gh/justadudewhohacks/face-api.js/models');
            await faceapi.nets.faceExpressionNet.loadFromUri('https://cdn.jsdelivr.net/gh/justadudewhohacks/face-api.js/models');
            console.log("Models Loaded!");
        }

        // Start webcam
        async function startWebcam() {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ video: true });
                video.srcObject = stream;
            } catch (error) {
                console.error("Error accessing webcam:", error);
            }
        }

        // Predict emotion
        async function predictEmotion() {
            if (!faceapi.nets.faceExpressionNet.params) {
                resultText.innerText = "Models are still loading...";
                return;
            }

            const detections = await faceapi.detectAllFaces(video, new faceapi.TinyFaceDetectorOptions()).withFaceExpressions();
            
            if (detections.length > 0) {
                const emotions = detections[0].expressions;
                const highestEmotion = Object.keys(emotions).reduce((a, b) => emotions[a] > emotions[b] ? a : b);
                resultText.innerText = `Emotion: ${highestEmotion}`;
            } else {
                resultText.innerText = "No face detected!";
            }
        }

        captureButton.addEventListener('click', predictEmotion);

        // Wait for models to load before starting
        loadModels().then(startWebcam);
    </script>

</body>
</html>
